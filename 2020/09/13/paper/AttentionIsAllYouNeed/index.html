<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>paper/AttentionIsAllYouNeed | Edlison</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Attention Is All You NeedAbstract我们提出一种全新的简洁的网络架构–Transfomer，它完全基于Attention机制，舍弃了循环与卷积。 1 IntroductionTransformer的并行性很好的解决了RNN模型的序列化输入输出。 2 Background3 Model Architecture大部分序列神经网络模型都具有encoder-decoder结">
<meta property="og:type" content="article">
<meta property="og:title" content="paper&#x2F;AttentionIsAllYouNeed">
<meta property="og:url" content="http://edlison.com/blog/2020/09/13/paper/AttentionIsAllYouNeed/index.html">
<meta property="og:site_name" content="Edlison">
<meta property="og:description" content="Attention Is All You NeedAbstract我们提出一种全新的简洁的网络架构–Transfomer，它完全基于Attention机制，舍弃了循环与卷积。 1 IntroductionTransformer的并行性很好的解决了RNN模型的序列化输入输出。 2 Background3 Model Architecture大部分序列神经网络模型都具有encoder-decoder结">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-09-13T12:32:41.635Z">
<meta property="article:modified_time" content="2020-09-16T14:16:17.047Z">
<meta property="article:author" content="Bolin Shen">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/blog/atom.xml" title="Edlison" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/blog/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Edlison</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">coding...</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/blog/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://edlison.com/blog"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-paper/AttentionIsAllYouNeed" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2020/09/13/paper/AttentionIsAllYouNeed/" class="article-date">
  <time datetime="2020-09-13T12:32:41.635Z" itemprop="datePublished">2020-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      paper/AttentionIsAllYouNeed
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>我们提出一种全新的简洁的网络架构–Transfomer，它完全基于Attention机制，舍弃了循环与卷积。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Transformer的并行性很好的解决了RNN模型的序列化输入输出。</p>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><p>大部分序列神经网络模型都具有encoder-decoder结构。</p>
<h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p>encoder由6个相同的层构成，每一层有两个子层。第一个是一个多Attention机制，第二个是一个简单的全连接feed-forward网络。两个子层中间部署一个参差网络，后接一个正则化。</p>
<p>decoder也是由6个相同的层构成。除了两个子层外中间还加入了一个第三子层Multi-head Attention.</p>
<h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>Attention的核心内容是为输入向量的每个单词(512d)学习一个权重。在self-attention中，每个单词有3个不同的向量，Query，Key，Value，长度均是64. 他们通过3个不同的权值矩阵由嵌入向量X乘以三个不同的权值矩阵WQ，WK，WV(512 * 64)得到。</p>
<p>Attention计算步骤：</p>
<ul>
<li>将输入单词转换成词向量</li>
<li>根据词向量得到q, k, v三个向量</li>
<li>为每个向量计算一个score: score = q * kT</li>
<li>为了梯度的稳定，Transformer使用了score归一化，即除以根号dk</li>
<li>对score过一遍softmax函数</li>
<li>softmax点乘value得到每个输入向量的评分v</li>
<li>想家之后得到最终的输出结果z=∑v</li>
</ul>
<h3 id="3-3-Multi-Head-Attention"><a href="#3-3-Multi-Head-Attention" class="headerlink" title="3.3 Multi-Head Attention"></a>3.3 Multi-Head Attention</h3><p>Multi-Head Attention相当于h个不同的self-attention的集成。</p>
<p>Multi-Head输出分三步</p>
<ul>
<li>将X分别输入到8个self-attention中得到8个加权后的特征矩阵Z</li>
<li>将8个Z按列拼接成一个大的特征矩阵</li>
<li>将特征矩阵经过一层fc后得到输出Z</li>
</ul>
<h3 id="3-4-损失层"><a href="#3-4-损失层" class="headerlink" title="3.4 损失层"></a>3.4 损失层</h3><p>解码器解码后，解码的特征向量通过一层激活函数为softmax的全连接层之后得到反应每个单词概率的输出向量。此时通过CTC等损失函数训练模型。</p>
<h2 id="4-位置编码"><a href="#4-位置编码" class="headerlink" title="4 位置编码"></a>4 位置编码</h2><p>Transfomer无法捕捉循序序列的能力。论文在编码词向量时引入了位置编码的特征，位置编码会在词向量中加入单词的位置信息。</p>
<p>在上式中， [公式] 表示单词的位置， [公式] 表示单词的维度。关于位置编码的实现可在Google开源的算法中get_timing_signal_1d()函数找到对应的代码。</p>
<p>作者这么设计的原因是考虑到在NLP任务重，除了单词的绝对位置，单词的相对位置也非常重要。根据公式 [公式] 以及[公式] ，这表明位置 [公式] 的位置向量可以表示为位置 [公式] 的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>位置编码对词袋模型等具有普适性？<br>encoder/decoder通过self-attention层所得到的向量即理解为加密/解密？<br>得到最后的Z之后怎么用？</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://edlison.com/blog/2020/09/13/paper/AttentionIsAllYouNeed/" data-id="ckf3hvm1y0001mwojhyqlfqah" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2020/09/15/notebook/docker-lineloss/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          notebook/docker-lineloss
        
      </div>
    </a>
  
  
    <a href="/blog/2020/08/16/notebook/Authentication&Authorization/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">notebook/Authentication&amp;Authorization</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/06/">六月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/03/">三月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2020/09/16/notebook/implements-extends-2/">notebook/implements-extends-2</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/15/notebook/docker-lineloss/">notebook/docker-lineloss</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/13/paper/AttentionIsAllYouNeed/">paper/AttentionIsAllYouNeed</a>
          </li>
        
          <li>
            <a href="/blog/2020/08/16/notebook/Authentication&Authorization/">notebook/Authentication&amp;Authorization</a>
          </li>
        
          <li>
            <a href="/blog/2020/08/02/nlp/word2vec/">nlp/word2vec</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Bolin Shen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">

  
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>




<script src="/blog/js/script.js"></script>




  </div>
</body>
</html>