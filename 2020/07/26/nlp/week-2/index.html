<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>nlp/week-2 | Edlison</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Week 2深度学习与神经网络基础，pytorch基础。 神经网络训练过程 将DataSet生成iter(train_iter, test_iter), 每个iter由X,y构成 (X:Sample的所有特征[samples * features], y:真实标签). 计算y_hat (y_hat:通过构造的神经网络模型得到的预测值). 计算损失函数 (均方差, 交叉熵). 对所有params梯度">
<meta property="og:type" content="article">
<meta property="og:title" content="nlp&#x2F;week-2">
<meta property="og:url" content="http://edlison.com/blog/2020/07/26/nlp/week-2/index.html">
<meta property="og:site_name" content="Edlison">
<meta property="og:description" content="Week 2深度学习与神经网络基础，pytorch基础。 神经网络训练过程 将DataSet生成iter(train_iter, test_iter), 每个iter由X,y构成 (X:Sample的所有特征[samples * features], y:真实标签). 计算y_hat (y_hat:通过构造的神经网络模型得到的预测值). 计算损失函数 (均方差, 交叉熵). 对所有params梯度">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-07-26T10:28:47.408Z">
<meta property="article:modified_time" content="2020-08-14T04:10:32.196Z">
<meta property="article:author" content="Bolin Shen">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/blog/atom.xml" title="Edlison" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/blog/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/blog/" id="logo">Edlison</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/blog/" id="subtitle">coding...</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/blog/">Home</a>
        
          <a class="main-nav-link" href="/blog/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/blog/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://edlison.com/blog"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-nlp/week-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/blog/2020/07/26/nlp/week-2/" class="article-date">
  <time datetime="2020-07-26T10:28:47.408Z" itemprop="datePublished">2020-07-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      nlp/week-2
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><p>深度学习与神经网络基础，pytorch基础。</p>
<h2 id="神经网络训练过程"><a href="#神经网络训练过程" class="headerlink" title="神经网络训练过程"></a>神经网络训练过程</h2><ul>
<li>将DataSet生成iter(train_iter, test_iter), 每个iter由X,y构成 (X:Sample的所有特征[samples * features], y:真实标签).</li>
<li>计算y_hat (y_hat:通过构造的神经网络模型得到的预测值).</li>
<li>计算损失函数 (均方差, 交叉熵).</li>
<li>对所有params梯度清零.</li>
<li>loss.backward反向传播, 对损失函数求梯度, 也就是得所有的到权重和偏置的梯度param.grad.</li>
<li>params带入梯度下降算法, 更新params.</li>
<li>每个epoch后, 将测试数据带入训练后的网络模型中计算准确率.</li>
</ul>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p><strong>1. W初始化</strong><br>首先介绍一下我们不应该做的事情（即初始化为0）。需要注意的是我们并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为0这个一个听起来还蛮合理的想法也许是一个我们假设中最好的一个假设了。但结果正确是一个错误(的想法)，因为如果神经网络计算出来的输出值都一个样，那么反向传播算法计算出来的梯度值一样，并且参数更新值也一样(w=w−α∗dw)。更一般地说，如果权重初始化为同一个值，网络就不可能不对称(即是对称的)。</p>
<p><strong>2. 梯度清零</strong><br>每次反向传播前需要将params的梯度清零，否则每次计算的梯度为上一个梯度的累加。</p>
<p><strong>3. 反向传播</strong><br>计算的是给定图的叶子结点的梯度，对Loss函数进行反向传播计算所得即为所有的W与b。</p>
<p><strong>4. python方法</strong><br>method()代表执行完该函数<br>method仅函数对象</p>
<p><strong>5. 二分类问题不小心将最后一层设为10维</strong><br>最后训练出来的权重使得结果在0, 1上的概率更大，不排除最后的结果为2 ~ 9。</p>
<p><strong>6. 网络的层次</strong><br>每一层为激活函数</p>
<p>层数-1 为连接层 连接层对应[W, b]</p>
<p>X = [batch_size * features]  </p>
<ul>
<li><p>第一层输入为([-1, features])  </p>
</li>
<li><p>矩阵计算: [-1, features] * [W1(input_num, hidden_num)] + b1  </p>
</li>
<li><p>隐藏层输入为(input_num, hidden_num)  </p>
</li>
<li><p>矩阵计算: [-1, hidden_num] * [W2(hidden_num, output_num)] + b2  </p>
</li>
<li><p>输出层为(hidden_num, output_num)  </p>
</li>
<li><p>最终矩阵格式: [-1, output_num]</p>
</li>
</ul>
<p><strong>7. 特殊第一层input输入层</strong><br>输入的特征可能type为Long, 一定要转为float.<br>batch_size与input_num无关, 但features一定与input_num相等.</p>
<p><strong>8. X.permute</strong><br>model的forward必须要X.permute(1, 0)<br><code>LSTM(batch_first=True)</code>没用？？？</p>
<p><strong>9. python路径问题</strong><br>相对路径取决于调用该函数的python文件位置，不是子函数所在python文件相对于文件的位置。</p>
<p><strong>10. pytorch使用GPU计算</strong>  </p>
<ul>
<li>训练及评价集需要X.cuda()将数据移至GPU  </li>
<li>model与loss函数需要model.cuda(), loss.cuda()移至GPU</li>
</ul>
<p><strong>11. 导出数据</strong><br>TestLoader导出预测结果时，一定不能将导入的数据Shuffle!!!</p>
<p><strong>12. pad</strong><br><code>&lt;pad&gt;</code>对应一个随机生成的向量</p>
<p><strong>13. torch.no_grad()</strong>  </p>
<ul>
<li>requires_grad=True 要求计算梯度</li>
<li>requires_grad=False 不要求计算梯度</li>
<li>with torch.no_grad()或者@torch.no_grad()中的数据不需要计算梯度，也不会进行反向传播</li>
</ul>
<p>即使一个tensor（命名为x）的requires_grad = True，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">10</span>, <span class="number">5</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = torch.randn(<span class="number">10</span>, <span class="number">5</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = torch.randn(<span class="number">10</span>, <span class="number">5</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    w = x + y + z</span><br><span class="line">    print(w.requires_grad)</span><br><span class="line">    print(w.grad_fn)</span><br><span class="line">    print(w.requires_grad)</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">None</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>

<p><strong>14. train()/eval()</strong>  </p>
<ul>
<li><p>在训练模型时，前面加上<code>model.train()</code><br>启用 BatchNormalization 和 Dropout</p>
</li>
<li><p>在测试模型时，前面加上<code>model.eval()</code><br>不启用 BatchNormalization 和 Dropout<br>即使不训练，它也会改变权值。这是model中含有batch normalization层所带来的的性质。</p>
</li>
</ul>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>最后一层输出不加激活函数 含义(最后预测值是数值最大的 因此没有影响)？？？</p>
<p>隐藏层没有激活函数 含义？？？</p>
<p><code>LSTM(batch_first=True)</code>没用？？？</p>
<p>embedding_layer(X)中X.shape为(batch,seq)或(seq,batch)没影响？？？<br>若batch_first了embedding后要(batch,seq)后传入rnn？？？</p>
<p>lstm门使用sigmoid代表门开程度，而不是四舍五入为0或1？？？</p>
<h2 id="词向量框架"><a href="#词向量框架" class="headerlink" title="词向量框架"></a>词向量框架</h2><ul>
<li>拿到每个样本</li>
<li>每个样本分词</li>
<li>构建词典{word: index}</li>
<li>对每个Sample标准化处理， 长度不足补<code>&lt;pad&gt;</code></li>
<li>将每个Sample中的word对应index，没有的对应<code>&lt;pad&gt;</code>的index</li>
<li>获取预训练的词向量</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://edlison.com/blog/2020/07/26/nlp/week-2/" data-id="ckd3h9jf00001rcojegdped30" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/blog/2020/08/02/nlp/word2vec/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          nlp/word2vec
        
      </div>
    </a>
  
  
    <a href="/blog/2020/07/26/notebook/docker-nextcloud/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">notebook/docker-nextcloud</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/09/">九月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/08/">八月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/07/">七月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/06/">六月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2020/03/">三月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/blog/2020/09/23/nlp/BERT/">nlp/BERT</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/23/nlp/Transformer/">nlp/Transformer</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/18/paper/Attention-Transformer/">paper/Attention-Transformer</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/18/paper/XLNet/">paper/XLNet</a>
          </li>
        
          <li>
            <a href="/blog/2020/09/16/notebook/implements-extends-2/">notebook/implements-extends-2</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Bolin Shen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/blog/" class="mobile-nav-link">Home</a>
  
    <a href="/blog/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css">

  
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>




<script src="/blog/js/script.js"></script>




  </div>
</body>
</html>